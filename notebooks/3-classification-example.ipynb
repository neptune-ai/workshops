{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import neptune\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_data(logs):\n",
    "    neptune.log_metric('epoch_accuracy', logs['accuracy'])\n",
    "    neptune.log_metric('epoch_loss', logs['loss'])\n",
    "\n",
    "\n",
    "def lr_scheduler(epoch):\n",
    "    if epoch < 20:\n",
    "        new_lr = PARAMS['learning_rate']\n",
    "    else:\n",
    "        new_lr = PARAMS['learning_rate'] * np.exp(0.05 * (20 - epoch))\n",
    "\n",
    "    neptune.log_metric('learning_rate', new_lr)\n",
    "    return new_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select project\n",
    "neptune.init('neptune-workshops/AII-Optimali')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "PARAMS = {'batch_size': 64,\n",
    "          'n_epochs': 100,\n",
    "          'shuffle': True,\n",
    "          'activation': 'elu',\n",
    "          'dense_units': 128,\n",
    "          'dropout': 0.2,\n",
    "          'learning_rate': 0.001,\n",
    "          'early_stopping': 10,\n",
    "          'optimizer': 'Adam',\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create experiment\n",
    "with neptune.create_experiment(name='classification_example',\n",
    "                               tags=['classification', 'tf_2'],\n",
    "                               params=PARAMS):\n",
    "    # Dataset\n",
    "    fashion_mnist = keras.datasets.fashion_mnist\n",
    "    (train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "    train_images = train_images / 255.0\n",
    "    test_images = test_images / 255.0\n",
    "\n",
    "    neptune.set_property('train_images_version', hashlib.md5(train_images).hexdigest())\n",
    "    neptune.set_property('train_labels_version', hashlib.md5(train_labels).hexdigest())\n",
    "    neptune.set_property('test_images_version', hashlib.md5(test_images).hexdigest())\n",
    "    neptune.set_property('test_labels_version', hashlib.md5(test_labels).hexdigest())\n",
    "\n",
    "    class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "                   'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "    neptune.set_property('class_names', class_names)\n",
    "\n",
    "    for j, class_name in enumerate(class_names):\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        label_ = np.where(train_labels == j)\n",
    "        for i in range(9):\n",
    "            plt.subplot(3, 3, i + 1)\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "            plt.grid(False)\n",
    "            plt.imshow(train_images[label_[0][i]], cmap=plt.cm.binary)\n",
    "            plt.xlabel(class_names[j])\n",
    "        neptune.log_image('example_images', plt.gcf())\n",
    "\n",
    "    # Model\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        keras.layers.Dense(PARAMS['dense_units'], activation=PARAMS['activation']),\n",
    "        keras.layers.Dropout(PARAMS['dropout']),\n",
    "        keras.layers.Dense(PARAMS['dense_units'], activation=PARAMS['activation']),\n",
    "        keras.layers.Dropout(PARAMS['dropout']),\n",
    "        keras.layers.Dense(PARAMS['dense_units'], activation=PARAMS['activation']),\n",
    "        keras.layers.Dropout(PARAMS['dropout']),\n",
    "        keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    if PARAMS['optimizer'] == 'Adam':\n",
    "        optimizer = tf.keras.optimizers.Adam(\n",
    "            learning_rate=PARAMS['learning_rate'],\n",
    "        )\n",
    "    elif PARAMS['optimizer'] == 'Nadam':\n",
    "        optimizer = tf.keras.optimizers.Nadam(\n",
    "            learning_rate=PARAMS['learning_rate'],\n",
    "        )\n",
    "    elif PARAMS['optimizer'] == 'SGD':\n",
    "        optimizer = tf.keras.optimizers.SGD(\n",
    "            learning_rate=PARAMS['learning_rate'],\n",
    "        )\n",
    "\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # Log model summary\n",
    "    model.summary(print_fn=lambda x: neptune.log_text('model_summary', x))\n",
    "\n",
    "    # Train model\n",
    "    model.fit(train_images, train_labels,\n",
    "              batch_size=PARAMS['batch_size'],\n",
    "              epochs=PARAMS['n_epochs'],\n",
    "              shuffle=PARAMS['shuffle'],\n",
    "              callbacks=[keras.callbacks.LambdaCallback(on_epoch_end=lambda epoch, logs: log_data(logs)),\n",
    "                         keras.callbacks.EarlyStopping(patience=PARAMS['early_stopping'],\n",
    "                                                       monitor='accuracy',\n",
    "                                                       restore_best_weights=True),\n",
    "                         keras.callbacks.LearningRateScheduler(lr_scheduler)]\n",
    "              )\n",
    "\n",
    "    # Log model weights\n",
    "    with tempfile.TemporaryDirectory(dir='.') as d:\n",
    "        prefix = os.path.join(d, 'model_weights')\n",
    "        model.save_weights(os.path.join(prefix, 'model'))\n",
    "        for item in os.listdir(prefix):\n",
    "            neptune.log_artifact(os.path.join(prefix, item),\n",
    "                                 os.path.join('model_weights', item))\n",
    "\n",
    "    # Evaluate model\n",
    "    eval_metrics = model.evaluate(test_images, test_labels, verbose=0)\n",
    "    for j, metric in enumerate(eval_metrics):\n",
    "        neptune.log_metric('eval_' + model.metrics_names[j], metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neptune.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
